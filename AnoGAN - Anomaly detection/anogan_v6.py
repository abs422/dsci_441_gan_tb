# -*- coding: utf-8 -*-
"""AnoGAN_v4 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LFN9AtmsU35VdYEjb6tFGcZ_sie1LN0j
"""

from __future__ import print_function
import tensorflow as tf
from keras.models import Sequential, Model
from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten
from keras.layers import Conv2DTranspose, LeakyReLU
from keras.layers.core import Activation
from keras.layers import BatchNormalization
from keras.optimizers import Adam, RMSprop
from keras import backend as K
from keras import initializers

import numpy as np
from tqdm import tqdm
import cv2
import math

from keras.utils. generic_utils import Progbar

from google.colab import drive
drive.mount('/content/drive')

img_size = 256
z_dim = 10

### combine images for visualization
def combine_images(generated_images):
    num = generated_images.shape[0]
    width = int(math.sqrt(num))
    height = int(math.ceil(float(num)/width))
    shape = generated_images.shape[1:4]
    image = np.zeros((height*shape[0], width*shape[1], shape[2]),
                     dtype=generated_images.dtype)
    for index, img in enumerate(generated_images):
        i = int(index/width)
        j = index % width
        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]
    return image

### generator model define
def generator_model(z_dim = z_dim, imgsize = img_size, channels = 1): 
    col = int(imgsize / 4)
    inputs = Input((z_dim, ))
    fc1 = Dense(input_dim=z_dim, units=128*col*col)(inputs)
    fc1 = BatchNormalization()(fc1)
    fc1 = LeakyReLU(0.2)(fc1)

    fc2 = Reshape((col, col, 128), input_shape=(128*col*col,))(fc1)
    up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)
    conv1 = Conv2D(64, (3,3), padding='same')(up1)
    conv1 = BatchNormalization()(conv1)
    conv1 = Activation('relu')(conv1)

    up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)
    conv2 = Conv2D(channels, (5, 5), padding='same')(up2)
    outputs = Activation('tanh')(conv2)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.summary()

    return model

def discriminator_model(img_size = img_size, channels = 1):
    inputs = Input((img_size, img_size, channels))

    conv1 = Conv2D(64, (5,5), padding='same')(inputs)
    conv1 = LeakyReLU(0.2)(conv1)
    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)

    conv2 = Conv2D(128, (5,5), padding='same')(pool1)
    conv2 = LeakyReLU(0.2)(conv2)
    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)

    fc1 = Flatten()(pool2)
    fc1 = Dense(1)(fc1)
    outputs = Activation('sigmoid')(fc1)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.summary()

    return model

### d_on_g model for training generator
def generator_containing_discriminator(g, d, z_dim = z_dim):
    d.trainable = False

    ganInput = Input(shape=(z_dim, ))
    x = g(ganInput)
    ganOutput = d(x)
    gan = Model(inputs=ganInput, outputs=ganOutput)

    return gan

def load_model():
    d = discriminator_model()
    g = generator_model()
    d_optim = RMSprop()
    g_optim = RMSprop(lr=0.0001)
    g.compile(loss='binary_crossentropy', optimizer=g_optim)
    d.compile(loss='binary_crossentropy', optimizer=d_optim)
    d.load_weights('./weights/discriminator.h5')
    g.load_weights('./weights/generator.h5')
    return g, d

### train generator and discriminator
def train(BATCH_SIZE, X_train, z_dim=z_dim):
    
    ### model define
    d = discriminator_model()
    g = generator_model()
    d_on_g = generator_containing_discriminator(g, d)
    d_optim = RMSprop(lr = 0.0004)
    g_optim = RMSprop(lr = 0.0002)
    g.compile(loss='mse', optimizer=g_optim)
    d_on_g.compile(loss='mse', optimizer=g_optim)
    d.trainable = True
    d.compile(loss='mse', optimizer=d_optim)
    

    for epoch in range(5):
        print ("Epoch is", epoch)
        n_iter = int(X_train.shape[0]/BATCH_SIZE)
        progress_bar = Progbar(target=n_iter)
        
        for index in range(n_iter):
            # create random noise -> U(0,1) 10 latent vectors
            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, z_dim))
            # noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 256))

            # load real data & generate fake data
            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]
            generated_images = g.predict(noise, verbose=0)
            
            # visualize training results
            if index % 20 == 0:
                image = combine_images(generated_images)
                image = image*127.5+127.5
                cv2.imwrite('./result/'+str(epoch)+"_"+str(index)+".png", image)

            # attach label for training discriminator
            X = np.concatenate((image_batch, generated_images))
            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)
            
            # training discriminator
            d_loss = d.train_on_batch(X, y)

            # training generator
            d.trainable = False
            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))
            d.trainable = True

            progress_bar.update(index, values=[('g',g_loss), ('d',d_loss)])
        print ('')

        # save weights for each epoch
        g.save_weights('weights/generator.h5', True)
        d.save_weights('weights/discriminator.h5', True)
    return d, g

### generate images
def generate(BATCH_SIZE, z_dim=z_dim):
    g = generator_model()
    g.load_weights('weights/generator.h5')
    noise = np.random.uniform(0, 1, (BATCH_SIZE, z_dim))
    # noise = np.random.uniform(0, 1, (BATCH_SIZE, 256))
    generated_images = g.predict(noise)
    return generated_images

### anomaly loss function 
def sum_of_residual(y_true, y_pred):
    return K.sum(K.abs(K.cast(y_true, dtype='float32') - K.cast(y_pred, dtype='float32')))

### discriminator intermediate layer feautre extraction
def feature_extractor(d=None):
    if d is None:
        d = discriminator_model()
        d.load_weights('weights/discriminator.h5') 
    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-7].output)
    intermidiate_model.compile(loss='binary_crossentropy', optimizer='rmsprop')
    return intermidiate_model

### anomaly detection model define
def anomaly_detector(g=None, d=None, z_dim=z_dim):
    if g is None:
        g = generator_model()
        g.load_weights('weights/generator.h5')
    intermidiate_model = feature_extractor(d)
    intermidiate_model.trainable = False
    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)
    g.trainable = False
    # Input layer cann't be trained. Add new layer as same size & same distribution
    aInput = Input(shape=(z_dim,))
    gInput = Dense((10), trainable=True)(aInput)
    gInput = Activation('sigmoid')(gInput)
    
    # G & D feature
    G_out = g(gInput)
    D_out= intermidiate_model(G_out)    
    model = Model(inputs=aInput, outputs=[G_out, D_out])
    model.compile(loss=sum_of_residual, loss_weights= [0.90, 0.10], optimizer='rmsprop')
    
    # batchnorm learning phase fixed (test) : make non trainable
    K.set_learning_phase(0)
    
    return model

### anomaly detection
def compute_anomaly_score(model, x, iterations=500, d=None, z_dim=z_dim):
    z = np.random.uniform(0, 1, size=(1, z_dim))
    
    intermidiate_model = feature_extractor(d)
    d_x = intermidiate_model.predict(x)

    # learning for changing latent
    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)
    similar_data, _ = model.predict(z)
    
    loss = loss.history['loss'][-1]
    
    return loss, similar_data

pip install PyQt5

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function

import matplotlib
matplotlib.use('Agg')
# import matplotlib.pyplot as plt
import os
import cv2
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from keras.datasets import mnist
import argparse

import argparse
# import anogan
import h5py

os.chdir('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images')

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

if not os.path.isdir("/content/drive/MyDrive/DSCI 441 - Anomaly detection in images"):
	os.mkdir("/content/drive/MyDrive/DSCI 441 - Anomaly detection in images")

# parser = argparse.ArgumentParser()
# parser.add_argument('--img_idx', type=int, default=14)
# parser.add_argument('--label_idx', type=int, default=7)
# parser.add_argument('--mode', type=str, default='test', help='train, test')
# args = parser.parse_args()
# parser.parse_args()

from PIL import Image

# Define path to image directory in Google Drive
norm_img_path = '/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/Data/TBX11K/imgs/health'
norm_img_path_mont = '/content/drive/MyDrive/Classifier_input_org_img/normal'
ptb_img_path = '/content/drive/MyDrive/Classifier_input_org_img/ptb'

# Load train image data from directory
data = []
for file in os.listdir(norm_img_path):
    img = Image.open(os.path.join(norm_img_path, file)).convert('L')
    img = img.resize((256, 256))
    data.append(np.array(img).reshape(256, 256, 1))

for file in os.listdir(norm_img_path_mont):
    img = Image.open(os.path.join(norm_img_path_mont, file)).convert('L')
    img = img.resize((256, 256))
    data.append(np.array(img).reshape(256, 256, 1))

X_train = np.array(data)

# Load test image data from directory
data = []
for file in os.listdir(ptb_img_path):
    img = Image.open(os.path.join(ptb_img_path, file)).convert('L')
    img = img.resize((256, 256))
    data.append(np.array(img).reshape(256, 256, 1))

X_test = np.array(data)

print("The shape of train set", X_train.shape)
print("The shape of test set", X_test.shape)
X_test_original = X_test.copy()

image = X_train[0]
print(image.shape)
first_img = np.reshape(image, (256, 256))

# Plot image
plt.imshow(first_img, cmap='gray')
plt.show()

# from PIL import Image
# img = Image.open(os.path.join('/content/drive/MyDrive/Classifier_input_org_img/normal/', 'CHNCXR_0001_0.png')).convert('L')
# img = img.resize((512, 512))
# plt.imshow(img, cmap='gray')
# plt.show()

# ### 0. prepare data
# (X_train, y_train), (X_test, y_test) = mnist.load_data()
# X_train = (X_train.astype(np.float32) - 127.5) / 127.5
# X_test = (X_test.astype(np.float32) - 127.5) / 127.5

# X_train = X_train[:,:,:,None]
# X_test = X_test[:,:,:,None]

# X_test_original = X_test.copy()

# # X_train = X_train[y_train==1]
# # X_test = X_test[y_test==1]
# print ('train shape:', X_train.shape)
# print ('test shape:', X_test.shape)

### 1. train generator & discriminator

Model_d, Model_g = train(64, X_train)

### 2. test generator
generated_img = generate(25)
img = combine_images(generated_img)
img = (img*127.5)+127.5
img = img.astype(np.uint8)
img = cv2.resize(img, None, fx=4, fy=4, interpolation=cv2.INTER_NEAREST)

def anomaly_detection(test_img, img_size=img_size, g=None, d=None):
    model = anomaly_detector(g=g, d=d)
    ano_score, similar_img = compute_anomaly_score(model, test_img.reshape(1, img_size, img_size, 1), iterations=500, d=d)

    # anomaly area, 255 normalization
    np_residual = test_img.reshape(img_size,img_size,1) - similar_img.reshape(img_size,img_size,1)
    np_residual = (np_residual + 2)/4

    np_residual = (255*np_residual).astype(np.uint8)
    original_x = (test_img.reshape(img_size,img_size,1)*127.5+127.5).astype(np.uint8)
    similar_x = (similar_img.reshape(img_size,img_size,1)*127.5+127.5).astype(np.uint8)

    original_x_color = cv2.cvtColor(original_x, cv2.COLOR_GRAY2BGR)
    residual_color = cv2.applyColorMap(np_residual, cv2.COLORMAP_JET)
    show = cv2.addWeighted(original_x_color, 0.3, residual_color, 0.7, 0.)

    return ano_score, original_x, similar_x, show

Model_d.save('Model_d')
Model_g.save('Model_g')

tf.keras.models.save_model(Model_d,'Model_d.hdf5')
tf.keras.models.save_model(Model_g,'Model_g.hdf5')

# img_idx = int(input("Enter the value of img_idx: "))

# label_idx = int(input("Enter the value of label_idx: "))

# mode = str(input("Enter the value of mode (train/test): "))

image = X_test_original[0]
print(image.shape)
first_img = np.reshape(image, (256, 256))

original_test = (first_img.reshape(img_size,img_size,1)*127.5+127.5)
# Plot image
plt.imshow(original_test, cmap='gray')
plt.show()

X_test_original[0].shape

### compute anomaly score - sample from strange image
# img_idx = img_idx
# label_idx = label_idx
test_img = X_test_original[0]
# test_img = np.random.uniform(-1,1, (28,28,1))

start = cv2.getTickCount()
score, qurey, pred, diff = anomaly_detection(test_img)
time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000
# print ('%d label, %d : done'%(label_idx, img_idx), '%.2f'%score, '%.2fms'%time)
# cv2.imwrite('./qurey.png', qurey)
# cv2.imwrite('./pred.png', pred)
# cv2.imwrite('./diff.png', diff)

## matplot view
plt.figure(1, figsize=(3, 3))
plt.title('query image')
plt.imshow(qurey.reshape(256,256), cmap=plt.cm.gray)

print("anomaly score : ", score)
plt.figure(2, figsize=(3, 3))
plt.title('generated similar image')
plt.imshow(pred.reshape(256,256).astype(np.uint16), cmap=plt.cm.gray)

plt.figure(3, figsize=(3, 3))
plt.title('anomaly detection')
plt.imshow(cv2.cvtColor(diff,cv2.COLOR_BGR2RGB))
plt.show()

### 4. tsne feature view

### t-SNE embedding 
### generating anomaly image for test (radom noise image)

from sklearn.manifold import TSNE

random_image = np.random.uniform(0, 1, (100, 256, 256, 1))
print("random noise image")
plt.figure(4, figsize=(2, 2))
plt.title('random noise image')
plt.imshow(random_image[0].reshape(256,256), cmap=plt.cm.gray)

# intermidieate output of discriminator
model = feature_extractor()
feature_map_of_random = model.predict(random_image, verbose=1)
feature_map_of_minist = model.predict(X_test_original[:300], verbose=1)
feature_map_of_minist_1 = model.predict(X_test[:100], verbose=1)

# t-SNE for visulization
output = np.concatenate((feature_map_of_random, feature_map_of_minist, feature_map_of_minist_1))
output = output.reshape(output.shape[0], -1)
anomaly_flag = np.array([1]*100+ [0]*300)

X_embedded = TSNE(n_components=2).fit_transform(output)
plt.figure(5)
plt.title("t-SNE embedding on the feature representation")
plt.scatter(X_embedded[:100,0], X_embedded[:100,1], label='random noise(anomaly)')
plt.scatter(X_embedded[100:400,0], X_embedded[100:400,1], label='mnist(anomaly)')
plt.scatter(X_embedded[400:,0], X_embedded[400:,1], label='mnist(normal)')
plt.legend()
plt.show()

test_img = X_test_original[0]
# test_img = np.random.uniform(-1,1, (28,28,1))

start = cv2.getTickCount()
score, qurey, pred, diff = anomaly_detection(test_img)

ascore_test = []
for i in range(0, X_test_original.shape[0]):
  test_img = X_test_original[i]
  score, qurey, pred, diff = anomaly_detection(test_img)
  ascore_test = ascore_test + [score]

ascore_train = []
for i in range(0, X_train.shape[0]):
  train_img = X_train[i]
  score, qurey, pred, diff = anomaly_detection(train_img)
  ascore_train = ascore_train + [score]

"""### Model Evaluation"""

# Restore the weights
model_g, model_d = load_model()

# Define path to image directory in Google Drive
sick_img_path = '/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/Data/TBX11K/imgs/sick'
tb_img_path = '/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/Data/TBX11K/imgs/tb'

# Load train image data from directory
data = []
for file in os.listdir(sick_img_path):
    img = Image.open(os.path.join(sick_img_path, file)).convert('L')
    img = img.resize((256, 256))
    data.append(np.array(img).reshape(256, 256, 1))

# Load train image data from directory
tb = []
for file in os.listdir(tb_img_path):
    img = Image.open(os.path.join(tb_img_path, file)).convert('L')
    img = img.resize((256, 256))
    tb.append(np.array(img).reshape(256, 256, 1))

import pandas as pd
ascore_test = []
for i in range(0, X_test_original.shape[0]):
  test_img = X_test_original[i]
  score, qurey, pred, diff = anomaly_detection(test_img)
  ascore_test = ascore_test + [score]


# ascore_train = []
# for i in range(0, X_train.shape[0]):
#   train_img = X_train[i]
#   score, qurey, pred, diff = anomaly_detection(train_img)
#   ascore_train = ascore_train + [score]
#   ascore_traindf = pd.DataFrame(ascore_train)
#   ascore_traindf.to_csv('ascore_traindf.csv')


X_sick = np.array(data)
ascore_sick = []
for i in range(0, X_sick.shape[0]):
  train_img = X_sick[i]
  score, qurey, pred, diff = anomaly_detection(X_sick)
  ascore_sick = ascore_sick + [score]
  ascore_sickdf = pd.DataFrame(ascore_sick)
  ascore_sickdf.to_csv('ascore_sickdf.csv')


X_tb = np.array(tb)
ascore_sick = []
for i in range(0, X_tb.shape[0]):
  train_img = X_tb[i]
  score, qurey, pred, diff = anomaly_detection(X_tb)
  ascore_tb = ascore_tb + [score]
  ascore_tbdf = pd.DataFrame(ascore_tb)
  ascore_tbdf.to_csv('ascore_tbdf.csv')

import matplotlib.pyplot as plt
import pandas as pd
ascore_testdf = pd.DataFrame(ascore_test)
ascore_testdf['patient'] = 'unhealthy' 
ascore_traindf = pd.DataFrame(ascore_train)
ascore_traindf['patient'] = 'healthy'
model_eval = ascore_traindf.append(ascore_testdf, ignore_index=True)

model_eval.to_csv('model_eval.csv')
# ascore_testdf.head()
# plt.hist(ascore_test,  color = 'r', rwidth = 0.5)
# plt.hist(ascore_train,  color = 'b', rwidth = 0.6)
# plt.show()

# Read the data back
import pandas as pd

sick = pd.read_csv('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/ascore_sickdf.csv')
healthy = pd.read_csv('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/ascore_traindf.csv')
# healthy_tb = pd.read_csv('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/model_eval.csv')
tb = pd.read_csv('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/ascore_tbdf.csv')
testdf = pd.read_csv('/content/drive/MyDrive/DSCI 441 - Anomaly detection in images/ascore_testdf.csv')
tb.head()

# Plotting anomaly scores
import matplotlib.pyplot as plt
import seaborn as sns

# plt.hist(sick[:500]['0'],  color = 'r', rwidth = 0.5)
# plt.hist(healthy[:500]['0'],  color = 'b', rwidth = 0.6)
# # plt.hist(tb['0'],  color = 'g', rwidth = 0.6)
# plt.hist(testdf[:100]['0'],  color = 'y', rwidth = 0.6)
# plt.show()

fig, ax = plt.subplots()

ax = sns.kdeplot(data=healthy[500:]['0'], label='Healthy- TbX110K', ax=ax, shade = True, color= 'red')
ax = sns.kdeplot(data=testdf[100:]['0'], label='Tb-Monto-Shenzhen' , ax=ax, shade = True, color = 'darkblue')
# ax = sns.kdeplot(data=tb[100:]['0'], label='Tb-TBX1100K', ax=ax)

# ax.set_xlabel('KDE plot example from seaborn")
ax.set_xlabel('Value')
ax.set_ylabel('Density')
ax.set_title('Comparison of Healthy vs Tuberculosis patients')

plt.show()

threhold = 9500000

import numpy as np

sick['label'] = np.where(sick['0'] > threhold, 1, 0)
sick['gt'] = 1

healthy['label'] = np.where(healthy['0'] > threhold, 1, 0)
healthy['gt'] = 0

testdf['label'] = np.where(testdf['0'] > threhold, 1, 0)
testdf['gt'] = 1

tb['label'] = np.where(tb['0'] > threhold, 1, 0)
tb['gt'] = 1

sick.head()

"""# Finding performance metrices"""

from sklearn import metrics
import matplotlib.pyplot as plt
def performance(df):
  predicted = df['label']
  actual = df['gt']
  confusion_matrix = metrics.confusion_matrix(actual, predicted)

  Accuracy = metrics.accuracy_score(actual, predicted)
  Precision = metrics.precision_score(actual, predicted)
  Sensitivity_recall = metrics.recall_score(actual, predicted)
  Specificity = metrics.recall_score(actual, predicted, pos_label=0)
  F1_score = metrics.f1_score(actual, predicted)
  #metrics
  return print({"Accuracy":Accuracy,"Precision":Precision,"Sensitivity_recall":Sensitivity_recall,"Specificity":Specificity,"F1_score":F1_score})

performance(healthy)

performance(sick)

performance(testdf)